
ACCESS_CODE = "12345"  # Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø¯Ø®Ø§Ù„Ù‡

from flask import Flask, render_template_string, request
from playwright.sync_api import sync_playwright, Page

import base64 
import pytesseract
#import requests
from io import BytesIO
import re
import threading
import webbrowser

import torch
import time
import cv2
import numpy as np
from sentence_transformers import SentenceTransformer, util

#from PIL import ImageChops
# Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ø¨Ù†Ù…ÙˆØ°Ø¬ Ù‚ÙˆÙŠ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¹Ù†ÙˆÙŠØ©
transformer_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

from PIL import Image
from datetime import datetime
import os
from rembg import remove

import imagehash






pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"



   
def extract_text_with_boxes(image):
    if image is None:
        print("ğŸš« ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© (image is None)")
        return []

    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØµÙ†Ø§Ø¯ÙŠÙ‚
        data = pytesseract.image_to_data(image_rgb, lang='eng+ara', output_type=pytesseract.Output.DICT)

        all_texts = []
        grouped_blocks = []  # â† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¶Ø§ÙØ©: Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù„ÙˆÙƒØ§Øª Ø§Ù„Ù†ØµÙŠØ©

        current_block_num = -1
        current_block_text = []

        # Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for i in range(len(data['level'])):
            block_num = data['block_num'][i]
            text = data['text'][i].strip()

            # ØªØ®Ø·ÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ÙØ§Ø±Øº
            if not text:
                continue

            if block_num != current_block_num:
                if current_block_text:
                    block_text = ' '.join(current_block_text)
                    all_texts.append(block_text)
                    grouped_blocks.append(current_block_text.copy())  # â† Ø£Ø¶Ù Ø§Ù„Ø¨Ù„ÙˆÙƒ ÙƒÙ‚Ø§Ø¦Ù…Ø©
                current_block_text = [text]
                current_block_num = block_num
            else:
                current_block_text.append(text)

        if current_block_text:
            block_text = ' '.join(current_block_text)
            all_texts.append(block_text)
            grouped_blocks.append(current_block_text.copy())  # â† Ø£Ø¶Ù Ø¢Ø®Ø± Ø¨Ù„ÙˆÙƒ

        print(f"ğŸ“¦ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(all_texts)}")

        # ÙŠÙ…ÙƒÙ†Ùƒ Ù‡Ù†Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§ØŒ Ø£Ùˆ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ grouped_blocks Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
        # return all_texts, grouped_blocks â† ÙÙŠ Ø­Ø§Ù„ Ø£Ø±Ø¯Øª Ø§Ù„Ø¥Ø«Ù†ÙŠÙ† Ù…Ø¹Ù‹Ø§
        return all_texts, grouped_blocks
  # â† Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª ÙÙ‚Ø· all_texts ÙƒÙ…Ø§ Ù‡Ùˆ Ø¸Ø§Ù‡Ø±

    except Exception as e:
        print("â— Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:", e)
        return []



    except Exception as e:
        print(f"â— Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}")
        return []



def extract_and_match_posts(all_texts, keywords, similarity_threshold):
    results = []

    # âœ… ØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ø§Ù‹ Ø£Ù† all_texts ØµØ§Ù„Ø­Ø©
    if not all_texts:
        print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§ (all_texts is empty or None).")
        return []

    

    # ØªØ£ÙƒØ¯ Ø£Ù† ÙƒÙ„ Ø¹Ù†ØµØ± Ù†ØµÙŠ
    try:
        texts = [t.strip() for t in all_texts if isinstance(t, str)]
    except Exception as e:
        print(f"â— Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ: {e}")
        return []

    ##print("....texts....", texts)
    ##input(".......")

    for line in texts:
        if line.strip() == "":
            continue

        #
        #print(f"\n--- Individual Line ---\n{line}\n")

        for kw in keywords:
            sim = compute_similarity(kw, line)

            if sim >= similarity_threshold:
                results.append({
                    "matched_keyword": kw,
                    "similarity": sim,
                    "text": line
                })
                break  # âœ… ØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ Ø£ÙˆÙ„ ØªØ·Ø§Ø¨Ù‚ Ù†Ø§Ø¬Ø­

    return results




def split_into_blocks (text):
    return [block.strip() for block in re.split(r'\n\s*\n', text) if block.strip()]

def is_similar_ai(block, keywords, threshold=0.6):
    best_similarity = 0.0
    best_keyword = None
    for keyword in keywords:
        similarity = compute_similarity(block, keyword)
        if similarity > best_similarity:
            best_similarity = similarity
            best_keyword = keyword
    if best_similarity >= threshold:
        return True, best_similarity, best_keyword
    return False, 0.0, None

def extract_matching_blocks(text, keywords):
    blocks = split_into_blocks(text)
    matched = []
    for block in blocks:
        match, sim, keyword = is_similar_ai(block, keywords)
        if match:
            matched.append((block, sim, keyword))
    return matched


# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)







def screenshots_are_similar(img_bytes1, img_bytes2, threshold=0.98):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª Ø¥Ù„Ù‰ ØµÙˆØ± PIL
    img1 = Image.open(BytesIO(img_bytes1)).convert("RGB")
    img2 = Image.open(BytesIO(img_bytes2)).convert("RGB")

    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ perceptual hash Ù„ÙƒÙ„Ø§ Ø§Ù„ØµÙˆØ±ØªÙŠÙ†
    hash1 = imagehash.phash(img1)
    hash2 = imagehash.phash(img2)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù‡Ø§Ø´Ø§Øª
    diff = hash1 - hash2
    hash_size = len(hash1.hash)**2  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ØªØ§Øª ÙÙŠ Ø§Ù„Ù‡Ø§Ø´ (Ø¹Ø§Ø¯Ø© 64)
    
    similarity = 1 - (diff / hash_size)

    #print("ğŸ” pHash similarity:", similarity)
    #input("Ø§Ø¶ØºØ· Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©...")

    return similarity >= threshold



def apply_mask_to_image(image_bytes, mask):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† bytes Ø¥Ù„Ù‰ NumPy array Ø¨ØµÙŠØºØ© BGR (Ù„Ù€ OpenCV)
    image_stream = BytesIO(image_bytes)
    pil_image = Image.open(image_stream).convert("RGB")
    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

    # Ø§Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ø§Ù„Ù‚Ù†Ø§Ø¹
    if image.shape[:2] != mask.shape:
        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))

    # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ù„ÙÙŠØ© Ø¨ÙŠØ¶Ø§Ø¡
    background = np.ones_like(image, dtype=np.uint8) * 255

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¥Ù„Ù‰ Ø«Ù„Ø§Ø« Ù‚Ù†ÙˆØ§Øª
    if len(mask.shape) == 2:
        mask_3ch = cv2.merge([mask, mask, mask])
    else:
        mask_3ch = mask

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚Ù†Ø§Ø¹
    foreground = cv2.bitwise_and(image, mask_3ch)
    background_masked = cv2.bitwise_and(background, cv2.bitwise_not(mask_3ch))
    final = cv2.add(foreground, background_masked)

    return final



def remove_background_with_rembg(image_bytes):
    # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Bytes ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ RGB
    input_image = Image.open(BytesIO(image_bytes)).convert("RGB")
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ©
    output_image = remove(input_image)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© NumPy
    output_np = np.array(output_image)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‚Ù†Ø§Ø© Alpha ÙƒÙ‚Ù†Ø§Ø¹ Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯
    if output_np.shape[2] == 4:
        mask = output_np[:, :, 3]
    else:
        mask = np.ones(output_np.shape[:2], dtype=np.uint8) * 255

    return mask


def is_keyword_match(keyword, text):
    if keyword in text:
        return 1.0  # ØªØ·Ø§Ø¨Ù‚ ØªØ§Ù…
    similarity = compute_similarity(keyword, text)
    return similarity


def compute_similarity(text1, text2, threshold=0.6):
    # ØªÙ‚Ø³ÙŠÙ… text2 Ø¥Ù„Ù‰ Ø¬Ù…Ù„ Ø£Ùˆ Ù…Ù‚Ø§Ø·Ø¹ ÙØ±Ø¹ÙŠØ©
    
    chunks = re.split(r'[.ØŸ!\n]', text2)
    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]

    # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ (text1 + ÙƒÙ„ Ø£Ø¬Ø²Ø§Ø¡ text2)
    texts = [text1] + chunks
    embeddings = transformer_model.encode(texts, convert_to_tensor=True)

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† text1 ÙˆÙƒÙ„ Ø¬Ø²Ø¡ Ù…Ù† text2
    similarities = util.pytorch_cos_sim(embeddings[0], embeddings[1:])
    max_similarity = similarities.max().item()

    #print("...max similarity with parts of text2......", max_similarity)
    return max_similarity





def save_cookies(context):
    cookies = context.cookies()
    with open("cookies.json", "w", encoding="utf-8") as f:
        import json
        json.dump(cookies, f)

# Ø¨Ø¹Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ ÙŠØ¯ÙˆÙŠÙ‹Ø§ ÙÙŠ Ù…ØªØµÙØ­ ØºÙŠØ± headless:


def load_cookies(context):
    import json, os
    if os.path.exists("cookies.json"):
        with open("cookies.json", "r", encoding="utf-8") as f:
            cookies = json.load(f)
            context.add_cookies(cookies)


def expand_all_facebook_posts(page):
    try:
        elements = page.query_selector_all('div[role="button"]')
        for el in elements:
            try:
                text = el.inner_text().strip()
                if text in ["See more", "Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯"]:
                    el.scroll_into_view_if_needed()
                    page.wait_for_timeout(200)
                    el.click()
                    page.wait_for_timeout(300)
            except:
                continue
    except Exception as e:
        print("âŒ Ø®Ø·Ø£:", e)



def analyze_posts_via_screenshot_ai(page: Page, keywords, max_posts, max_scrolls, similarity_threshold):

    seen_texts = set()
    scroll_count = 0
    analyzed_count = 0
    posts_data = []
    last_screenshot = None
    

    while analyzed_count < max_posts and scroll_count < max_scrolls:
        
        expand_all_facebook_posts(page)



        # Ø«Ù… Ù†Ø£Ø®Ø° screenshot ÙƒØ§Ù„Ù…Ø¹ØªØ§Ø¯
        screenshot_bytes = page.screenshot(full_page=False)




      
        if last_screenshot and screenshots_are_similar(screenshot_bytes, last_screenshot):
            print("[Scroll Detection] Screenshot is similar. Skipping...")
            scroll_count += 1
            page.keyboard.press("PageDown")
            page.wait_for_timeout(1500)
            continue
        else:
            #print("[Scroll Detection] First screenshot.")
            last_screenshot = screenshot_bytes

        masked_image = None  # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ØªØºÙŠØ± Ù‚Ø¨Ù„ try

        try:
            mask = remove_background_with_rembg(screenshot_bytes)
            masked_image = apply_mask_to_image(screenshot_bytes, mask)
        except Exception as e:
            print("Mask extraction failed:", e)
            try:
                image_stream = BytesIO(screenshot_bytes)
                pil_image = Image.open(image_stream).convert("RGB")
                masked_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            except Exception as e2:
                print("Fallback failed:", e2)
                masked_image = None

        
        if masked_image is not None:
            _, buffer = cv2.imencode('.png', masked_image)
            image_base64 = base64.b64encode(buffer).decode("utf-8")
        else:
            image_base64 = ""


        all_texts, grouped_blocks = extract_text_with_boxes(masked_image)

        #print("...all text...",all_texts)
        #input(".....")

        for i, full_text in enumerate(all_texts):

            #print("...full text...",full_text)
            #input("......")
            for keyword in keywords:
                similarity = compute_similarity(full_text.strip(), keyword.strip())
                #print("...similarity....",similarity)
                #input("......")
                if similarity >= similarity_threshold:

                    
                    
                    matched_block = grouped_blocks[i]
                    #print("âœ… Bloc Ù…Ø·Ø§Ø¨Ù‚:", ' '.join(matched_block))
                    #input(".....")

                   # ØªØ±Ù…ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ base64
                    image_base64 = base64.b64encode(screenshot_bytes).decode("utf-8")

                    posts_data.append({
                        "text": ' '.join(matched_block),
                        "matched_keyword": keyword,
                        "similarity": f"{similarity:.2f}",
                        "image_base64": image_base64,
                        "page_url": page.url
                    })

                    break


        analyzed_count += 1
        if analyzed_count < max_posts:
            page.keyboard.press("PageDown")
            page.wait_for_timeout(1500)

    posts_data.sort(key=lambda x: x.get("similarity", 0), reverse=True)


    return posts_data




# Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø¨Ø¯ÙˆÙ† Ø·Ø¨Ø§Ø¹Ø©
def run_facebook_scraper(keywords, facebook_pages, max_posts, similarity_threshold):

    max_scrolls=5
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        
        load_cookies(context)
        page = context.new_page()
        
        all_results = []
       
        for fb_page in facebook_pages:
            page.goto(fb_page, timeout=60000)
            time.sleep(5)

           
            # âœ… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø®Ø§Ø±Ø¬ Ø§Ù„Ù€ try
            posts_data = analyze_posts_via_screenshot_ai(page, keywords, max_posts, max_scrolls, similarity_threshold)


            all_results.extend(posts_data)

        browser.close()

        #cleanup_static_images()

        return all_results

def cleanup_static_images():
    folder = "static"
    for filename in os.listdir(folder):
        if filename.startswith("match_") and filename.endswith(".png"):
            os.remove(os.path.join(folder, filename))




# ÙˆØ§Ø¬Ù‡Ø© HTML Ø¨Ø³ÙŠØ·Ø©
HTML_TEMPLATE = '''
<!DOCTYPE html>
<html lang="en">
<head>

<script>
function openImage(src) {
  const w = window.open("", "_blank", "width=900,height=600,resizable=yes");
  w.document.write(`
    <html><head><title>Enlarged Image</title></head>
    <body style="margin:0;display:flex;justify-content:center;align-items:center;background:#000;">
      <img src="${src}" style="max-width:100%;max-height:100%;">
    </body></html>
  `);
}
</script>

    <meta charset="UTF-8">
    <title>Facebook Post Analyzer</title>
</head>
<body>
    <h2>Analyze Facebook Posts   /  malaz janbeih-malazjanbeih@gmail.com-+96170647081  </h2>
    <form method="POST">
        <label>Facebook Links (one per line):</label><br>
        <textarea name="links" rows="10" cols="60" style="border:1px solid #ccc; padding:5px; line-height:1.5; font-family: monospace; white-space: pre-wrap;"></textarea><br><br>
        <label>Keywords (separated by -):</label><br>
        <input type="text" name="keywords" size="60"><br><br>
        <label>Max Posts per Link:</label><br>
        <input type="number" name="max_posts" min="1" placeholder="Enter max posts"><br><br>
        <label>Similarity Threshold (default 0.7):</label><br>
        <input type="number" name="similarity" step="0.01" min="0.1" max="1.0" value="0.7"><br><br>
        <label>Enter Access Code:</label><br>
        <input type="text" name="access_code" required><br><br>

        <input type="submit" value="Analyze">
    </form>

   {% if message %}
    <p style="color:red;"><strong>{{ message }}</strong></p>
    {% endif %}

    {% for r in results %}
        <li>
            <strong>Matched Keyword:</strong> {{ r.matched_keyword }}<br>
            <strong>Text:</strong> <pre>{{ r.text }}</pre><br>
            <img src="data:image/png;base64,{{ r.image_base64 }}" width="300"
                style="cursor: zoom-in; border: 1px solid #ccc; border-radius: 5px;"
                onclick="openImage(this.src)">
        </li><hr>
    {% endfor %}

</body>
</html>
'''

@app.route("/", methods=["GET", "POST"])
def home():
    message = ""
    results = []

    if request.method == "POST":
        access_code = request.form.get("access_code", "").strip()
        if access_code != ACCESS_CODE:
            message = "âŒ Incorrect Access Code."
            return render_template_string(HTML_TEMPLATE, message=message)

        links = request.form["links"].strip().splitlines()
        keywords = request.form["keywords"].strip().split("-")
        max_posts = int(request.form["max_posts"])
        similarity_input = request.form.get("similarity", "0.7")
        try:
            similarity_threshold = float(similarity_input)
        except:
            similarity_threshold = 0.7

        results_holder = {}
        thread = threading.Thread(
            target=lambda: results_holder.update({
                "results": run_facebook_scraper(keywords, links, max_posts, similarity_threshold)
            })
        )
        thread.start()
        thread.join()
        results = results_holder.get("results", [])
        if not results:
            message = "â— No matched keyword found."
        return render_template_string(HTML_TEMPLATE, results=results, message=message)

    return render_template_string(HTML_TEMPLATE)


def open_browser():
    time.sleep(1)
    webbrowser.open("http://127.0.0.1:5000")

if __name__ == "__main__":
    threading.Thread(target=open_browser).start()
    app.run(debug=False)